{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UQC6YTGMOkux"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "5XPbSs_oOx8h",
        "outputId": "626c6c23-c76b-48c0-f9fa-0e1915e2ba55"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9KKS6flPI2-"
      },
      "outputs": [],
      "source": [
        " ! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Fz2NHlQPNmE"
      },
      "outputs": [],
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRIw7wGPPWQ9"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQCDMofNPdCb",
        "outputId": "68dc06c1-77ac-41c7-8ab3-7d8b59943026"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSXcChpUPpxA",
        "outputId": "1a60891e-d103-465a-b934-0831de011e74"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d odins0n/ucf-crime-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBCARKVrRR6O",
        "outputId": "c1a8cb68-9fa8-4b60-e7ca-33a10195fbab"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "zip_path= \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\ucf-crime-dataset.zip\"\n",
        "\n",
        "with ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('C:\\\\Users\\\\user\\\\Desktop\\\\raushan')\n",
        "print('unzipping completed')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.26.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jeA2AG3BUCo0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define a function to collect all frame paths and their respective labels\n",
        "def load_dataset(root_dir):\n",
        "    classes = os.listdir(root_dir)\n",
        "    frame_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for class_idx, class_name in enumerate(classes):\n",
        "        class_path = os.path.join(root_dir, class_name)\n",
        "        images = glob(os.path.join(class_path, \"*.png\"))\n",
        "        frame_paths.extend(images)\n",
        "        labels.extend([class_idx] * len(images))\n",
        "\n",
        "    return frame_paths, labels\n",
        "\n",
        "# Load train and test datasets\n",
        "train_root_dir = \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\Train\"\n",
        "test_root_dir = \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\Test\"\n",
        "\n",
        "train_frame_paths, train_labels = load_dataset(train_root_dir)\n",
        "test_frame_paths, test_labels = load_dataset(test_root_dir)\n",
        "\n",
        "# Optionally split the train data for validation\n",
        "train_frame_paths, val_frame_paths, train_labels, val_labels = train_test_split(\n",
        "    train_frame_paths, train_labels, test_size=0.1, random_state=42,train_size=0.9\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYxLw9MBVCsL",
        "outputId": "2f007598-34b0-43c3-8928-8a46f53da98d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1139710\n",
            "126635\n",
            "111308\n"
          ]
        }
      ],
      "source": [
        "print(len(train_frame_paths))\n",
        "print(len(val_frame_paths))\n",
        "print(len(test_frame_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5AWeI4ElrAF",
        "outputId": "84929799-e3cf-432e-ec3e-84be5bc2d28f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes found: ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fighting', 'NormalVideos', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']\n",
            "Found 19076 frames in Abuse\n",
            "Found 26397 frames in Arrest\n",
            "Found 24421 frames in Arson\n",
            "Found 10360 frames in Assault\n",
            "Found 39504 frames in Burglary\n",
            "Found 18753 frames in Explosion\n",
            "Found 24684 frames in Fighting\n",
            "Found 947768 frames in NormalVideos\n",
            "Found 23486 frames in RoadAccidents\n",
            "Found 41493 frames in Robbery\n",
            "Found 7140 frames in Shooting\n",
            "Found 24835 frames in Shoplifting\n",
            "Found 44802 frames in Stealing\n",
            "Found 13626 frames in Vandalism\n",
            "Total sequences created: 79141\n",
            "Classes found: ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fighting', 'NormalVideos', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']\n",
            "Found 297 frames in Abuse\n",
            "Found 3365 frames in Arrest\n",
            "Found 2793 frames in Arson\n",
            "Found 2657 frames in Assault\n",
            "Found 7657 frames in Burglary\n",
            "Found 6510 frames in Explosion\n",
            "Found 1231 frames in Fighting\n",
            "Found 64952 frames in NormalVideos\n",
            "Found 2663 frames in RoadAccidents\n",
            "Found 835 frames in Robbery\n",
            "Found 7630 frames in Shooting\n",
            "Found 7623 frames in Shoplifting\n",
            "Found 1984 frames in Stealing\n",
            "Found 1111 frames in Vandalism\n",
            "Total sequences created: 6950\n",
            "Train dataset length: 79141\n",
            "Test dataset length: 6950\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from glob import glob\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define a custom dataset to handle the UCF Crime frames and group them into sequences\n",
        "class FrameSequenceDataset(Dataset):\n",
        "    def __init__(self, root_dir, frame_count=16, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.frame_count = frame_count\n",
        "        self.transform = transform\n",
        "        self.classes = os.listdir(root_dir)\n",
        "        self.frame_sequences = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Debug: Print the classes found\n",
        "        print(\"Classes found:\", self.classes)\n",
        "\n",
        "        # Iterate over each class and gather frame sequences\n",
        "        for class_idx, class_name in enumerate(self.classes):\n",
        "            class_path = os.path.join(root_dir, class_name)\n",
        "            frames = sorted(glob(os.path.join(class_path, \"*.png\")))\n",
        "            print(f\"Found {len(frames)} frames in {class_name}\")  # Debug: Check frames in each class\n",
        "\n",
        "            # Group frames into sequences of self.frame_count\n",
        "            for i in range(0, len(frames), self.frame_count):\n",
        "                frame_seq = frames[i:i + self.frame_count]\n",
        "                if len(frame_seq) == self.frame_count:\n",
        "                    self.frame_sequences.append(frame_seq)\n",
        "                    self.labels.append(class_idx)\n",
        "\n",
        "        # Debug: Print the number of sequences created\n",
        "        print(\"Total sequences created:\", len(self.frame_sequences))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frame_sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        frame_seq_paths = self.frame_sequences[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load and preprocess frames\n",
        "        frames = [Image.open(frame_path).convert(\"RGB\") for frame_path in frame_seq_paths]\n",
        "        if self.transform:\n",
        "            frames = [self.transform(frame) for frame in frames]\n",
        "\n",
        "        # Stack the frames (TimeSformer expects them as a tensor)\n",
        "        frames = torch.stack(frames)\n",
        "\n",
        "        return frames, torch.tensor(label)\n",
        "\n",
        "# Image transforms (resize to 224x224 for TimeSformer)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    #transforms.RandomHoizontalFlip(),\n",
        "    transforms.ColorJitter(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load the train dataset\n",
        "train_dataset = FrameSequenceDataset(root_dir=\"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\Train\", frame_count=16, transform=transform)\n",
        "#val_dataset = FrameSequenceDataset(root_dir=\"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\val\", frame_count=16, transform=transform)  # Remove if not needed\n",
        "test_dataset = FrameSequenceDataset(root_dir=\"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\Test\", frame_count=16, transform=transform)\n",
        "\n",
        "# DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "#val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)  # Remove if not needed\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Check the lengths of the datasets\n",
        "print(\"Train dataset length:\", len(train_dataset))\n",
        "# print(\"Validation dataset length:\", len(val_dataset))  # Remove if not needed\n",
        "print(\"Test dataset length:\", len(test_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19786"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- timesformer.embeddings.position_embeddings: found shape torch.Size([1, 197, 768]) in the checkpoint and torch.Size([1, 65, 768]) in the model instantiated\n",
            "- timesformer.embeddings.patch_embeddings.projection.weight: found shape torch.Size([768, 3, 16, 16]) in the checkpoint and torch.Size([768, 3, 8, 8]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([14, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoModelForVideoClassification\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
        "config.image_size = 64\n",
        "config.num_labels = 14\n",
        "config.patch_size = 8\n",
        "\n",
        "model = AutoModelForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\", config =config, ignore_mismatched_sizes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "ab0e40a656df47c9a7eeb3b661c30a9e",
            "b1b317ee181c40c8a6747c42f7b7b93b",
            "b705ae74ea354e45980fd45fe04f2ce0",
            "543ded25c4b04a1bae801316b41e7b14",
            "1a99f2603c0241488694a0ba3a7f395b",
            "019243e7224f4e248b46135e2193a267",
            "8758105677f24949ad39c3aa650a165a",
            "44889dc130534b3dae0704d39094959c",
            "31d21d02b8ce43acb2968082191ec8ce",
            "ae1a06c80fab4bc29fa587c4c128718c",
            "a7fa1c194d4048a388e84c8c3342de29",
            "398196f92fe3458dbbd5c91a2aa563fc",
            "4a95b4b48428482584e5e93998ec294d",
            "e9bf0034dea1412aafafed56e81bf084",
            "99bcf45f49ea465280fd15430b317439",
            "8de70ed3a3ec4ef2b3ebf29112a990c9",
            "2212e87e4fa246bfb80fc3de5709a792",
            "80260a16422c4ff8962bb836552dac13",
            "5fbf44d3fb1e468986d7e1c2b64a3f7c",
            "a99f77b726cc4a4a9fd50a3e51960fa9",
            "08390d94f7984820a4f3081bab293101",
            "9e348447ac2d431aa4f20d65c7483a0c",
            "a70404df53a544b4a53fe4a9d0d36c48",
            "9e446983144145f38f3fe6a8696f7e74",
            "1f445d2cbe164eac9943b9169e05a066",
            "ffeac10b9ea34e66bc5b68302529eafc",
            "333ec5cd772843fe8fc041eca37e8d89",
            "22cf733bc0524ac4ab466e0089c03cea",
            "7839e369c8b5439d82a3ede210e93556",
            "b87464a1460d4ff4b21b428fbeaf7aae",
            "73d355f2b37b40e0a7e61dbccc3c436f",
            "d3322e17cba64302a6fd52c6ddd7b462",
            "c3d3b5b0473b4243a781da67a0e7bc32"
          ]
        },
        "id": "K4Kl667PpKls",
        "outputId": "70a8ffb1-f994-4064-adce-932641104e26"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoImageProcessor, AutoModelForVideoClassification\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
        "#model = AutoModelForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\",\n",
        "                                                      # num_labels=len(train_dataset.classes), # This will correctly set the output to 14 classes\n",
        "                                                      # ignore_mismatched_sizes=True) # This will handle the size mismatch in the classifier layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "a5vpeZjcmhxC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "import torch\n",
        "\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "'''\n",
        "# Freeze the first few layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the final classification head for fine-tuning\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
        "'''\n",
        "# Define optimizer and learning rate\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "OgiZLOoyrULS",
        "outputId": "60b1750c-9fa6-4569-d3cf-6c961277865f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - Training:   0%|          | 0/19786 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - Training: 100%|██████████| 19786/19786 [2:14:49<00:00,  2.45it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 0.9368\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - Validation: 100%|██████████| 1738/1738 [04:38<00:00,  6.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Val Loss: 2.0922\n",
            "Best model saved at epoch 1 to C:\\Users\\user\\Desktop\\raushan\\best_model_epoch_1.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 - Training: 100%|██████████| 19786/19786 [2:04:37<00:00,  2.65it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.6002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 - Validation: 100%|██████████| 1738/1738 [04:17<00:00,  6.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Val Loss: 2.2485\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 - Training: 100%|██████████| 19786/19786 [2:04:54<00:00,  2.64it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.3835\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 - Validation: 100%|██████████| 1738/1738 [04:18<00:00,  6.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Val Loss: 2.4470\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 - Training: 100%|██████████| 19786/19786 [2:03:33<00:00,  2.67it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.2503\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 - Validation: 100%|██████████| 1738/1738 [04:18<00:00,  6.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Val Loss: 2.6340\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 - Training: 100%|██████████| 19786/19786 [2:03:38<00:00,  2.67it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/10], Train Loss: 0.1778\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 - Validation: 100%|██████████| 1738/1738 [04:18<00:00,  6.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/10], Val Loss: 3.0518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 - Training: 100%|██████████| 19786/19786 [2:02:32<00:00,  2.69it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/10], Train Loss: 0.1307\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 - Validation: 100%|██████████| 1738/1738 [04:17<00:00,  6.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/10], Val Loss: 2.8561\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 - Training: 100%|██████████| 19786/19786 [2:01:52<00:00,  2.71it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/10], Train Loss: 0.1011\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 - Validation: 100%|██████████| 1738/1738 [04:17<00:00,  6.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/10], Val Loss: 2.9865\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 - Training: 100%|██████████| 19786/19786 [2:02:44<00:00,  2.69it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/10], Train Loss: 0.0810\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 - Validation: 100%|██████████| 1738/1738 [04:18<00:00,  6.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/10], Val Loss: 3.3868\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 - Training: 100%|██████████| 19786/19786 [2:02:57<00:00,  2.68it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/10], Train Loss: 0.0676\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 - Validation: 100%|██████████| 1738/1738 [04:17<00:00,  6.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/10], Val Loss: 3.7142\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 - Training: 100%|██████████| 19786/19786 [2:01:57<00:00,  2.70it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/10], Train Loss: 0.0555\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 - Validation: 100%|██████████| 1738/1738 [04:18<00:00,  6.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/10], Val Loss: 3.6198\n",
            "Training complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Initialize TensorBoard for visualizing the training and validation loss\n",
        "writer = SummaryWriter(log_dir=\"runs/experiment_1\")\n",
        "\n",
        "# Path where you want to save the model\n",
        "save_dir = \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\\"  # Your desired path\n",
        "\n",
        "# Make sure the save directory exists\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # Increased number of epochs for better training\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "# Ensure model is on the right device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training and validation loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
        "        frames, labels = batch\n",
        "        frames = frames.to(device)  # Move frames to GPU/CPU\n",
        "        labels = labels.to(device)  # Move labels to GPU/CPU\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(frames, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Log the training loss to TensorBoard\n",
        "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
        "            frames, labels = batch\n",
        "            frames = frames.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(frames, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(test_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Log the validation loss to TensorBoard\n",
        "    writer.add_scalar('Loss/validation', avg_val_loss, epoch)\n",
        "\n",
        "    # Save the best model based on validation loss\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        \n",
        "        # Save the model, optimizer, and epoch to a checkpoint\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_val_loss,\n",
        "        }\n",
        "        checkpoint_path = os.path.join(save_dir, f\"best_model_epoch_{epoch+1}.pth\")\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        print(f\"Best model saved at epoch {epoch+1} to {checkpoint_path}\")\n",
        "\n",
        "# Close the TensorBoard writer\n",
        "writer.close()\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- timesformer.embeddings.position_embeddings: found shape torch.Size([1, 197, 768]) in the checkpoint and torch.Size([1, 65, 768]) in the model instantiated\n",
            "- timesformer.embeddings.patch_embeddings.projection.weight: found shape torch.Size([768, 3, 16, 16]) in the checkpoint and torch.Size([768, 3, 8, 8]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([14, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - Training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - Training: 100%|██████████| 6484/6484 [45:06<00:00,  2.40it/s, loss=0.188]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Train Loss: 1.0305\n",
            "Epoch 1/10 - Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - Validation: 100%|██████████| 1621/1621 [04:03<00:00,  6.67it/s, loss=0.0539]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Val Loss: 0.8887\n",
            "Best model saved at epoch 1 to C://Users//user//Desktop//raushan/best_model_epoch_1.pth\n",
            "Epoch 2/10 - Training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 - Training: 100%|██████████| 6484/6484 [38:48<00:00,  2.78it/s, loss=0.0629] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Train Loss: 0.8076\n",
            "Epoch 2/10 - Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 - Validation: 100%|██████████| 1621/1621 [03:43<00:00,  7.24it/s, loss=0.0106] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Val Loss: 0.7775\n",
            "Best model saved at epoch 2 to C://Users//user//Desktop//raushan/best_model_epoch_2.pth\n",
            "Epoch 3/10 - Training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 - Training: 100%|██████████| 6484/6484 [38:15<00:00,  2.82it/s, loss=0.0982]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Train Loss: 0.6695\n",
            "Epoch 3/10 - Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 - Validation: 100%|██████████| 1621/1621 [03:59<00:00,  6.78it/s, loss=0.0045] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Val Loss: 0.6344\n",
            "Best model saved at epoch 3 to C://Users//user//Desktop//raushan/best_model_epoch_3.pth\n",
            "Epoch 4/10 - Training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 - Training: 100%|██████████| 6484/6484 [38:51<00:00,  2.78it/s, loss=0.647]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Train Loss: 0.5498\n",
            "Epoch 4/10 - Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 - Validation: 100%|██████████| 1621/1621 [04:02<00:00,  6.69it/s, loss=0.00689]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Val Loss: 0.5270\n",
            "Best model saved at epoch 4 to C://Users//user//Desktop//raushan/best_model_epoch_4.pth\n",
            "Epoch 5/10 - Training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 - Training: 100%|██████████| 6484/6484 [39:21<00:00,  2.75it/s, loss=2.25]    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/10], Train Loss: 0.4455\n",
            "Epoch 5/10 - Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 - Validation: 100%|██████████| 1621/1621 [04:00<00:00,  6.73it/s, loss=0.000576]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/10], Val Loss: 0.4327\n",
            "Best model saved at epoch 5 to C://Users//user//Desktop//raushan/best_model_epoch_5.pth\n",
            "Epoch 6/10 - Training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 - Training: 100%|██████████| 6484/6484 [38:51<00:00,  2.78it/s, loss=0.126]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/10], Train Loss: 0.3660\n",
            "Epoch 6/10 - Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 - Validation: 100%|██████████| 1621/1621 [03:44<00:00,  7.23it/s, loss=0.00259] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/10], Val Loss: 0.3715\n",
            "Best model saved at epoch 6 to C://Users//user//Desktop//raushan/best_model_epoch_6.pth\n",
            "Epoch 7/10 - Training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 - Training: 100%|██████████| 6484/6484 [39:43<00:00,  2.72it/s, loss=1.61]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/10], Train Loss: 0.3013\n",
            "Epoch 7/10 - Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 - Validation: 100%|██████████| 1621/1621 [04:04<00:00,  6.64it/s, loss=0.00055] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/10], Val Loss: 0.3070\n",
            "Best model saved at epoch 7 to C://Users//user//Desktop//raushan/best_model_epoch_7.pth\n",
            "Epoch 8/10 - Training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 - Training: 100%|██████████| 6484/6484 [39:11<00:00,  2.76it/s, loss=0.0172]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/10], Train Loss: 0.2432\n",
            "Epoch 8/10 - Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 - Validation: 100%|██████████| 1621/1621 [03:46<00:00,  7.17it/s, loss=0.00721] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/10], Val Loss: 0.3761\n",
            "Epoch 9/10 - Training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 - Training: 100%|██████████| 6484/6484 [38:37<00:00,  2.80it/s, loss=0.0711]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/10], Train Loss: 0.2036\n",
            "Epoch 9/10 - Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 - Validation: 100%|██████████| 1621/1621 [03:56<00:00,  6.86it/s, loss=0.0248]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/10], Val Loss: 0.2603\n",
            "Best model saved at epoch 9 to C://Users//user//Desktop//raushan/best_model_epoch_9.pth\n",
            "Epoch 10/10 - Training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 - Training: 100%|██████████| 6484/6484 [38:39<00:00,  2.80it/s, loss=0.444]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/10], Train Loss: 0.1690\n",
            "Epoch 10/10 - Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 - Validation: 100%|██████████| 1621/1621 [03:46<00:00,  7.16it/s, loss=0.000974]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/10], Val Loss: 0.2580\n",
            "Best model saved at epoch 10 to C://Users//user//Desktop//raushan/best_model_epoch_10.pth\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoModelForVideoClassification, AutoConfig\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Check device (GPU/CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load model configuration with 14 output classes\n",
        "#config = AutoConfig.from_pretrained(\"facebook/timesformer-base-finetuned-k400\", num_labels=14)\n",
        "\n",
        "# Load pre-trained model, handling shape mismatches\n",
        "model = AutoModelForVideoClassification.from_pretrained(\n",
        "    \"facebook/timesformer-base-finetuned-k400\",\n",
        "    config=config,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Replace classifier with a new one for 14 classes\n",
        "model.classifier = nn.Linear(model.config.hidden_size, 14)\n",
        "model.to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Define dataset (replace with actual dataset loading)\n",
        "# Example: train_dataset should be a PyTorch dataset of video frames and labels\n",
        "# train_dataset = YourCustomDataset()\n",
        "total_samples = len(train_dataset)\n",
        "val_split = int(0.2 * total_samples)  # 20% validation\n",
        "train_split = total_samples - val_split\n",
        "\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_split, val_split])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Output directory for saving models\n",
        "save_dir = \"C://Users//user//Desktop//raushan/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs} - Training:\")\n",
        "    train_bar = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch}/{num_epochs} - Training\")\n",
        "    \n",
        "    for frames, labels in train_bar:\n",
        "        frames, labels = frames.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(frames).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        train_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs} - Validation:\")\n",
        "    val_bar = tqdm(val_loader, total=len(val_loader), desc=f\"Epoch {epoch}/{num_epochs} - Validation\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for frames, labels in val_bar:\n",
        "            frames, labels = frames.to(device), labels.to(device)\n",
        "            outputs = model(frames).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            val_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}], Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_model_path = os.path.join(save_dir, f\"best_model_epoch_{epoch}.pth\")\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Best model saved at epoch {epoch} to {best_model_path}\")\n",
        "\n",
        "    # Save last epoch model\n",
        "    last_model_path = os.path.join(save_dir, \"last_epoch_model.pth\")\n",
        "    torch.save(model.state_dict(), last_model_path)\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()  # Clears unused memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- timesformer.embeddings.position_embeddings: found shape torch.Size([1, 197, 768]) in the checkpoint and torch.Size([1, 65, 768]) in the model instantiated\n",
            "- timesformer.embeddings.patch_embeddings.projection.weight: found shape torch.Size([768, 3, 16, 16]) in the checkpoint and torch.Size([768, 3, 8, 8]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([14, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/20 - Training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20 - Training: 100%|██████████| 5187/5187 [33:04<00:00,  2.61it/s, loss=0.000763]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [15/20], Train Loss: 0.0719\n",
            "Epoch 15/20 - Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20 - Validation: 100%|██████████| 1297/1297 [03:21<00:00,  6.42it/s, loss=1.19e-5] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [15/20], Val Loss: 0.0742\n",
            "Best model saved at epoch 15 to C://Users//user//Desktop//raushan/best_model_epoch_15.pth\n",
            "Epoch 16/20 - Training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20 - Training: 100%|██████████| 5187/5187 [30:15<00:00,  2.86it/s, loss=0.000432]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [16/20], Train Loss: 0.0603\n",
            "Epoch 16/20 - Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20 - Validation: 100%|██████████| 1297/1297 [03:30<00:00,  6.16it/s, loss=1.01e-6] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [16/20], Val Loss: 0.1071\n",
            "Epoch 17/20 - Training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20 - Training: 100%|██████████| 5187/5187 [31:36<00:00,  2.73it/s, loss=2.1e-5]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [17/20], Train Loss: 0.0564\n",
            "Epoch 17/20 - Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20 - Validation: 100%|██████████| 1297/1297 [03:09<00:00,  6.83it/s, loss=1.07e-5] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [17/20], Val Loss: 0.0646\n",
            "Best model saved at epoch 17 to C://Users//user//Desktop//raushan/best_model_epoch_17.pth\n",
            "Epoch 18/20 - Training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20 - Training: 100%|██████████| 5187/5187 [30:21<00:00,  2.85it/s, loss=0.0271]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [18/20], Train Loss: 0.0508\n",
            "Epoch 18/20 - Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20 - Validation: 100%|██████████| 1297/1297 [03:02<00:00,  7.10it/s, loss=8.34e-7] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [18/20], Val Loss: 0.0895\n",
            "Epoch 19/20 - Training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/20 - Training: 100%|██████████| 5187/5187 [30:16<00:00,  2.85it/s, loss=0.0181]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [19/20], Train Loss: 0.0473\n",
            "Epoch 19/20 - Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/20 - Validation: 100%|██████████| 1297/1297 [03:03<00:00,  7.08it/s, loss=1.73e-5] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [19/20], Val Loss: 0.0970\n",
            "Epoch 20/20 - Training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/20 - Training: 100%|██████████| 5187/5187 [31:05<00:00,  2.78it/s, loss=2.06e-6]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [20/20], Train Loss: 0.0421\n",
            "Epoch 20/20 - Validation:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/20 - Validation: 100%|██████████| 1297/1297 [03:18<00:00,  6.52it/s, loss=4.53e-6] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [20/20], Val Loss: 0.0830\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoModelForVideoClassification\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Check device (GPU/CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Path to the saved checkpoint (from epoch 10)\n",
        "checkpoint_path = \"C://Users//user//Desktop//raushan//best_model_epoch_13.pth\"\n",
        "\n",
        "# Load pre-trained model\n",
        "model = AutoModelForVideoClassification.from_pretrained(\n",
        "    \"facebook/timesformer-base-finetuned-k400\",\n",
        "    config=config,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "\n",
        "# Replace classifier with a new one for 14 classes (ensuring correct output shape)\n",
        "model.classifier = nn.Linear(model.config.hidden_size, 14)\n",
        "\n",
        "# Load the saved checkpoint\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "\n",
        "# Move model to the appropriate device\n",
        "model.to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Define dataset (ensure `train_dataset` is properly defined)\n",
        "total_samples = len(train_dataset)\n",
        "val_split = int(0.2 * total_samples)  # 20% validation\n",
        "train_split = total_samples - val_split\n",
        "\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_split, val_split])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Output directory for saving models\n",
        "save_dir = \"C://Users//user//Desktop//raushan/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Resume training from epoch 11\n",
        "start_epoch = 15\n",
        "num_epochs = 20\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs} - Training:\")\n",
        "    train_bar = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch}/{num_epochs} - Training\")\n",
        "\n",
        "    for frames, labels in train_bar:\n",
        "        frames, labels = frames.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(frames).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        train_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs} - Validation:\")\n",
        "    val_bar = tqdm(val_loader, total=len(val_loader), desc=f\"Epoch {epoch}/{num_epochs} - Validation\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for frames, labels in val_bar:\n",
        "            frames, labels = frames.to(device), labels.to(device)\n",
        "            outputs = model(frames).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            val_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}], Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_model_path = os.path.join(save_dir, f\"best_model_epoch_{epoch}.pth\")\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Best model saved at epoch {epoch} to {best_model_path}\")\n",
        "\n",
        "    # Save last epoch model\n",
        "    last_model_path = os.path.join(save_dir, \"last_epoch_model.pth\")\n",
        "    torch.save(model.state_dict(), last_model_path)\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.set_per_process_memory_fraction(0.9, device=0)  # Allocates 90% of GPU memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- timesformer.embeddings.position_embeddings: found shape torch.Size([1, 197, 768]) in the checkpoint and torch.Size([1, 65, 768]) in the model instantiated\n",
            "- timesformer.embeddings.patch_embeddings.projection.weight: found shape torch.Size([768, 3, 16, 16]) in the checkpoint and torch.Size([768, 3, 8, 8]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([14, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class labels: ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fighting', 'NormalVideos', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']\n",
            "\n",
            "Evaluating on Test Dataset...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing:  45%|████▍     | 782/1738 [02:47<02:24,  6.60it/s]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import AutoModelForVideoClassification, AutoConfig\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load model configuration\n",
        "config = AutoConfig.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
        "config.image_size = 64  # Set same as training\n",
        "config.num_labels = 14  # Number of classes\n",
        "config.patch_size = 8\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForVideoClassification.from_pretrained(\n",
        "    \"facebook/timesformer-base-finetuned-k400\",\n",
        "    config=config,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Load trained weights\n",
        "model_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\best_model_epoch_17.pth\"  # Change if using best model\n",
        "assert os.path.exists(model_path), f\"Model file not found: {model_path}\"\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define label mapping from dataset\n",
        "class_labels = os.listdir(\"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\Train\")  # Same as used in training\n",
        "print(\"Class labels:\", class_labels)\n",
        "\n",
        "# Initialize lists to store predictions and ground truths\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Evaluate on test dataset\n",
        "print(\"\\nEvaluating on Test Dataset...\\n\")\n",
        "with torch.no_grad():\n",
        "    for frames, labels in tqdm(test_loader, total=len(test_loader), desc=\"Testing\"):\n",
        "        frames, labels = frames.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(frames).logits  # Get model predictions\n",
        "        preds = torch.argmax(outputs, dim=1)  # Get predicted class indices\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert indices to class names\n",
        "predicted_labels = [class_labels[i] for i in all_preds]\n",
        "actual_labels = [class_labels[i] for i in all_labels]\n",
        "\n",
        "# Compute accuracy, precision, recall, and F1-score\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"weighted\")\n",
        "\n",
        "# Print results\n",
        "print(\"\\nTest Performance Metrics:\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "# Print a few predictions for reference\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(10):  # Print 10 sample predictions\n",
        "    if i < len(predicted_labels):\n",
        "        print(f\"Actual: {actual_labels[i]}  |  Predicted: {predicted_labels[i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:106: SyntaxWarning: invalid escape sequence '\\D'\n",
            "<>:106: SyntaxWarning: invalid escape sequence '\\D'\n",
            "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2800\\1228025479.py:106: SyntaxWarning: invalid escape sequence '\\D'\n",
            "  model_path = \"C:\\\\Users\\\\user\\Desktop\\\\raushan\\\\best_model_epoch_12.pth\"  # Change this path\n",
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- timesformer.embeddings.position_embeddings: found shape torch.Size([1, 197, 768]) in the checkpoint and torch.Size([1, 65, 768]) in the model instantiated\n",
            "- timesformer.embeddings.patch_embeddings.projection.weight: found shape torch.Size([768, 3, 16, 16]) in the checkpoint and torch.Size([768, 3, 8, 8]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([14, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Class Index: 3\n",
            "Predicted Label: Assault\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "from transformers import AutoModelForVideoClassification, AutoConfig\n",
        "\n",
        "# Set device (Use GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define class labels (Update this with your actual labels)\n",
        "CLASS_LABELS = {\n",
        "    0: \"Abuse\",\n",
        "    1: \"Arrest\",\n",
        "    2: \"Arson\",\n",
        "    3: \"Assault\",\n",
        "    4: \"Burglary\",\n",
        "    5: \"Explosion\",\n",
        "    6: \"Fighting\",\n",
        "    7: \"Normalvideos\",\n",
        "    8: \"RoadAccidents\",\n",
        "    9: \"Robbery\",\n",
        "    10: \"Shooting\",\n",
        "    11: \"Shoplifting\",\n",
        "    12: \"Stealing\",\n",
        "    13: \"Vandalism\"  # Example: Update these labels accordingly\n",
        "}\n",
        "\n",
        "# Load trained model function\n",
        "def load_trained_model(model_path):\n",
        "    # Load model config\n",
        "    config = AutoConfig.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
        "    config.image_size = 64  # Ensure it matches training config\n",
        "    config.num_labels = 14\n",
        "    config.patch_size = 8\n",
        "\n",
        "    # Load model with modified config\n",
        "    model = AutoModelForVideoClassification.from_pretrained(\n",
        "        \"facebook/timesformer-base-finetuned-k400\",\n",
        "        config=config,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "    \n",
        "    # Replace classifier with 14-class output\n",
        "    model.classifier = torch.nn.Linear(model.config.hidden_size, 14)\n",
        "    \n",
        "    # Load trained weights\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "# Function to extract frames from a video\n",
        "def extract_frames(video_path, num_frames=8, frame_size=(224, 224)):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    if total_frames == 0:\n",
        "        print(\"Error: Cannot read video.\")\n",
        "        return None\n",
        "\n",
        "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "    frames = []\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize(frame_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225])\n",
        "    ])\n",
        "\n",
        "    for i in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            continue\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = transform(frame)\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) < num_frames:\n",
        "        print(\"Error: Not enough frames extracted.\")\n",
        "        return None  # Return None if not enough frames are available\n",
        "\n",
        "    return torch.stack(frames)  # Shape: (num_frames, 3, 224, 224)\n",
        "\n",
        "# Function to predict class from a video\n",
        "def predict_on_video(video_path, model):\n",
        "    frames = extract_frames(video_path)\n",
        "    if frames is None:\n",
        "        return None\n",
        "\n",
        "    frames = frames.unsqueeze(0).to(device)  # Add batch dimension: (1, num_frames, 3, 224, 224)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(frames).logits  # Get model predictions\n",
        "        predicted_class_idx = torch.argmax(outputs, dim=1).item()  # Get predicted class index\n",
        "\n",
        "    # Get class label from dictionary\n",
        "    predicted_label = CLASS_LABELS.get(predicted_class_idx, \"Unknown\")\n",
        "\n",
        "    return predicted_class_idx, predicted_label\n",
        "\n",
        "# Load trained model\n",
        "model_path = \"C:\\\\Users\\\\user\\Desktop\\\\raushan\\\\best_model_epoch_12.pth\"  # Change this path\n",
        "model = load_trained_model(model_path)\n",
        "\n",
        "# Test on a video\n",
        "video_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\mixkit-strong-female-mixed-martial-arts-fighter-40991-hd-ready.mp4\"  # Change this path\n",
        "pred_class_idx, predicted_label = predict_on_video(video_path, model)\n",
        "\n",
        "if predicted_label is not None:\n",
        "    print(f\"Predicted Class Index: {pred_class_idx}\")\n",
        "    print(f\"Predicted Label: {predicted_label}\")\n",
        "else:\n",
        "    print(\"Prediction failed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 120.88 GiB. GPU 0 has a total capacity of 47.99 GiB of which 40.60 GiB is free. Of the allocated memory 5.64 GiB is allocated by PyTorch, and 136.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m model \u001b[38;5;241m=\u001b[39m load_trained_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124muser\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mraushan\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbest_model_epoch_10.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Load trained model\u001b[39;00m\n\u001b[0;32m     52\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124muser\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mraushan\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mvideoplayback (online-video-cutter.com).mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Change this to your video file path\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_on_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[11], line 45\u001b[0m, in \u001b[0;36mpredict_on_video\u001b[1;34m(video_path, model)\u001b[0m\n\u001b[0;32m     42\u001b[0m frames \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 45\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits  \u001b[38;5;66;03m# Get model output\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     pred_class \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Get predicted class\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred_class\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\timesformer\\modeling_timesformer.py:770\u001b[0m, in \u001b[0;36mTimesformerForVideoClassification.forward\u001b[1;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;124;03meating spaghetti\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m    768\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 770\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimesformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    777\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    779\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\timesformer\\modeling_timesformer.py:636\u001b[0m, in \u001b[0;36mTimesformerModel.forward\u001b[1;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    632\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m    634\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values)\n\u001b[1;32m--> 636\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    642\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\timesformer\\modeling_timesformer.py:442\u001b[0m, in \u001b[0;36mTimesformerEncoder.forward\u001b[1;34m(self, hidden_states, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    436\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    437\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    438\u001b[0m         hidden_states,\n\u001b[0;32m    439\u001b[0m         output_attentions,\n\u001b[0;32m    440\u001b[0m     )\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 442\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\timesformer\\modeling_timesformer.py:379\u001b[0m, in \u001b[0;36mTimesformerLayer.forward\u001b[1;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[0;32m    370\u001b[0m spatial_embedding \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m     spatial_embedding\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m    372\u001b[0m         batch_size, num_patch_height, num_patch_width, num_frames, spatial_embedding\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m num_frames, num_patch_height \u001b[38;5;241m*\u001b[39m num_patch_width, spatial_embedding\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m    376\u001b[0m )\n\u001b[0;32m    377\u001b[0m spatial_embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((cls_token, spatial_embedding), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 379\u001b[0m spatial_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayernorm_before\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspatial_embedding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m spatial_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    383\u001b[0m outputs \u001b[38;5;241m=\u001b[39m spatial_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\timesformer\\modeling_timesformer.py:249\u001b[0m, in \u001b[0;36mTimeSformerAttention.forward\u001b[1;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    246\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    247\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    248\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m--> 249\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    253\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\timesformer\\modeling_timesformer.py:209\u001b[0m, in \u001b[0;36mTimesformerSelfAttention.forward\u001b[1;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[0;32m    202\u001b[0m qkv \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv(hidden_states)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;241m.\u001b[39mreshape(batch_size, hidden_size, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, num_channels \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    206\u001b[0m )\n\u001b[0;32m    207\u001b[0m query, key, value \u001b[38;5;241m=\u001b[39m qkv[\u001b[38;5;241m0\u001b[39m], qkv[\u001b[38;5;241m1\u001b[39m], qkv[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m--> 209\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m (\u001b[43mquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[0;32m    210\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m attention_probs\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    211\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_drop(attention_probs)\n",
            "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 120.88 GiB. GPU 0 has a total capacity of 47.99 GiB of which 40.60 GiB is free. Of the allocated memory 5.64 GiB is allocated by PyTorch, and 136.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the trained model\n",
        "def load_trained_model(model_path=\"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\best_model_epoch_10.pth\"):\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    print(\"Model loaded successfully!\")\n",
        "    return model\n",
        "\n",
        "# Read video frames using OpenCV\n",
        "def read_video_opencv(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    \n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((64, 64)),  # Resize frames to match model input\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    \n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "        frames.append(transform(frame))\n",
        "    \n",
        "    cap.release()\n",
        "    \n",
        "    if len(frames) == 0:\n",
        "        raise ValueError(\"No frames were extracted from the video!\")\n",
        "    \n",
        "    return torch.stack(frames)\n",
        "\n",
        "# Predict the class of a video\n",
        "def predict_on_video(video_path, model):\n",
        "    frames = read_video_opencv(video_path)  # Extract frames\n",
        "    frames = frames.unsqueeze(0).to(device)  # Add batch dimension\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(frames).logits  # Get model output\n",
        "        pred_class = torch.argmax(outputs, dim=1).item()  # Get predicted class\n",
        "    \n",
        "    return pred_class\n",
        "\n",
        "# Example usage\n",
        "model = load_trained_model(\"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\best_model_epoch_10.pth\")  # Load trained model\n",
        "video_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\videoplayback (online-video-cutter.com).mp4\"  # Change this to your video file path\n",
        "predicted_class = predict_on_video(video_path, model)\n",
        "print(f\"Predicted Class: {predicted_class}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prediction on test dataset\n",
        "def predict_on_test():\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            frames, _ = batch\n",
        "            frames = frames.to(device)\n",
        "            outputs = model(frames).logits\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "            predictions.extend(preds)\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Video prediction function\n",
        "def predict_on_video(video_path):\n",
        "    frames, _, _ = read_video(video_path, pts_unit='sec')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    frames = torch.stack([transform(frame) for frame in frames])\n",
        "    frames = frames.unsqueeze(0).to(device)  # Add batch dimension\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(frames).logits\n",
        "        pred = torch.argmax(outputs, dim=1).item()\n",
        "    \n",
        "    return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "PyAV is not installed, and is necessary for the video operations in torchvision.\nSee https://github.com/mikeboers/PyAV#installation for instructions on how to\ninstall PyAV on your system.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpredict_on_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mraushan\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mmixkit-strong-female-mixed-martial-arts-fighter-40991-hd-ready.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[50], line 3\u001b[0m, in \u001b[0;36mpredict_on_video\u001b[1;34m(video_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_on_video\u001b[39m(video_path):\n\u001b[1;32m----> 3\u001b[0m     frames, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mread_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpts_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msec\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      5\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)),\n\u001b[0;32m      6\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      7\u001b[0m     ])\n\u001b[0;32m      8\u001b[0m     frames \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([transform(frame) \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames])\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\io\\video.py:319\u001b[0m, in \u001b[0;36mread_video\u001b[1;34m(filename, start_pts, end_pts, pts_unit, output_format)\u001b[0m\n\u001b[0;32m    317\u001b[0m     vframes, aframes, info \u001b[38;5;241m=\u001b[39m _video_opt\u001b[38;5;241m.\u001b[39m_read_video(filename, start_pts, end_pts, pts_unit)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 319\u001b[0m     \u001b[43m_check_av_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end_pts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    322\u001b[0m         end_pts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\io\\video.py:45\u001b[0m, in \u001b[0;36m_check_av_available\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_av_available\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(av, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[1;32m---> 45\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m av\n",
            "\u001b[1;31mImportError\u001b[0m: PyAV is not installed, and is necessary for the video operations in torchvision.\nSee https://github.com/mikeboers/PyAV#installation for instructions on how to\ninstall PyAV on your system.\n"
          ]
        }
      ],
      "source": [
        "predict_on_video(\"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\mixkit-strong-female-mixed-martial-arts-fighter-40991-hd-ready.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- timesformer.embeddings.position_embeddings: found shape torch.Size([1, 197, 768]) in the checkpoint and torch.Size([1, 65, 768]) in the model instantiated\n",
            "- timesformer.embeddings.patch_embeddings.projection.weight: found shape torch.Size([768, 3, 16, 16]) in the checkpoint and torch.Size([768, 3, 8, 8]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([14, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'C:\\\\Users\\\\user\\\\Desktop\\\\raushan'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[27], line 78\u001b[0m\n\u001b[0;32m     75\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Assuming you have a DataLoader for your test set (e.g., `test_loader`):\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Replace this with your actual test or validation DataLoader\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# test_loader = DataLoader(...)\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[27], line 22\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(checkpoint_path, config_path)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Load the checkpoint (fine-tuned weights)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path)\n\u001b[1;32m---> 22\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mraushan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "\u001b[1;31mKeyError\u001b[0m: 'C:\\\\Users\\\\user\\\\Desktop\\\\raushan'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoConfig, AutoModelForVideoClassification\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Function to load the model\n",
        "def load_model(checkpoint_path, config_path=\"facebook/timesformer-base-finetuned-k400\"):\n",
        "    # Load the configuration\n",
        "    config = AutoConfig.from_pretrained(config_path)\n",
        "    config.image_size = 64  # Ensure the image size matches your fine-tuned model's setting\n",
        "    config.num_labels = 14  # Number of output classes\n",
        "    config.patch_size = 8  # Patch size\n",
        "\n",
        "    # Load the model\n",
        "    model = AutoModelForVideoClassification.from_pretrained(config_path, config=config, ignore_mismatched_sizes=True)\n",
        "    \n",
        "    # Load the checkpoint (fine-tuned weights)\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['C:\\\\Users\\\\user\\\\Desktop\\\\raushan'])\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    return model\n",
        "\n",
        "# Evaluation function to compute accuracy, precision, recall, F1 score, and confusion matrix\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Iterate over the validation or test set\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            frames, labels = batch\n",
        "            frames = frames.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass: Get model predictions\n",
        "            outputs = model(frames)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=1)  # Get the class with the highest logit\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate accuracy, precision, recall, and F1 score\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "    \n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plot_confusion_matrix(cm)\n",
        "\n",
        "    return accuracy, precision, recall, f1, cm\n",
        "\n",
        "# Function to plot the confusion matrix\n",
        "def plot_confusion_matrix(cm, labels=None):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('True labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "checkpoint_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\best_model_epoch_11.pth\"  # Path to your fine-tuned model checkpoint\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the model\n",
        "model = load_model(checkpoint_path)\n",
        "model.to(device)\n",
        "\n",
        "# Assuming you have a DataLoader for your test set (e.g., `test_loader`):\n",
        "# Replace this with your actual test or validation DataLoader\n",
        "# test_loader = DataLoader(...)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy, precision, recall, f1, cm = evaluate_model(model, test_loader, device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- timesformer.embeddings.position_embeddings: found shape torch.Size([1, 197, 768]) in the checkpoint and torch.Size([1, 65, 768]) in the model instantiated\n",
            "- timesformer.embeddings.patch_embeddings.projection.weight: found shape torch.Size([768, 3, 16, 16]) in the checkpoint and torch.Size([768, 3, 8, 8]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([14, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint Keys: odict_keys(['timesformer.embeddings.cls_token', 'timesformer.embeddings.position_embeddings', 'timesformer.embeddings.time_embeddings', 'timesformer.embeddings.patch_embeddings.projection.weight', 'timesformer.embeddings.patch_embeddings.projection.bias', 'timesformer.encoder.layer.0.attention.attention.qkv.weight', 'timesformer.encoder.layer.0.attention.attention.qkv.bias', 'timesformer.encoder.layer.0.attention.output.dense.weight', 'timesformer.encoder.layer.0.attention.output.dense.bias', 'timesformer.encoder.layer.0.intermediate.dense.weight', 'timesformer.encoder.layer.0.intermediate.dense.bias', 'timesformer.encoder.layer.0.output.dense.weight', 'timesformer.encoder.layer.0.output.dense.bias', 'timesformer.encoder.layer.0.layernorm_before.weight', 'timesformer.encoder.layer.0.layernorm_before.bias', 'timesformer.encoder.layer.0.layernorm_after.weight', 'timesformer.encoder.layer.0.layernorm_after.bias', 'timesformer.encoder.layer.0.temporal_layernorm.weight', 'timesformer.encoder.layer.0.temporal_layernorm.bias', 'timesformer.encoder.layer.0.temporal_attention.attention.qkv.weight', 'timesformer.encoder.layer.0.temporal_attention.attention.qkv.bias', 'timesformer.encoder.layer.0.temporal_attention.output.dense.weight', 'timesformer.encoder.layer.0.temporal_attention.output.dense.bias', 'timesformer.encoder.layer.0.temporal_dense.weight', 'timesformer.encoder.layer.0.temporal_dense.bias', 'timesformer.encoder.layer.1.attention.attention.qkv.weight', 'timesformer.encoder.layer.1.attention.attention.qkv.bias', 'timesformer.encoder.layer.1.attention.output.dense.weight', 'timesformer.encoder.layer.1.attention.output.dense.bias', 'timesformer.encoder.layer.1.intermediate.dense.weight', 'timesformer.encoder.layer.1.intermediate.dense.bias', 'timesformer.encoder.layer.1.output.dense.weight', 'timesformer.encoder.layer.1.output.dense.bias', 'timesformer.encoder.layer.1.layernorm_before.weight', 'timesformer.encoder.layer.1.layernorm_before.bias', 'timesformer.encoder.layer.1.layernorm_after.weight', 'timesformer.encoder.layer.1.layernorm_after.bias', 'timesformer.encoder.layer.1.temporal_layernorm.weight', 'timesformer.encoder.layer.1.temporal_layernorm.bias', 'timesformer.encoder.layer.1.temporal_attention.attention.qkv.weight', 'timesformer.encoder.layer.1.temporal_attention.attention.qkv.bias', 'timesformer.encoder.layer.1.temporal_attention.output.dense.weight', 'timesformer.encoder.layer.1.temporal_attention.output.dense.bias', 'timesformer.encoder.layer.1.temporal_dense.weight', 'timesformer.encoder.layer.1.temporal_dense.bias', 'timesformer.encoder.layer.2.attention.attention.qkv.weight', 'timesformer.encoder.layer.2.attention.attention.qkv.bias', 'timesformer.encoder.layer.2.attention.output.dense.weight', 'timesformer.encoder.layer.2.attention.output.dense.bias', 'timesformer.encoder.layer.2.intermediate.dense.weight', 'timesformer.encoder.layer.2.intermediate.dense.bias', 'timesformer.encoder.layer.2.output.dense.weight', 'timesformer.encoder.layer.2.output.dense.bias', 'timesformer.encoder.layer.2.layernorm_before.weight', 'timesformer.encoder.layer.2.layernorm_before.bias', 'timesformer.encoder.layer.2.layernorm_after.weight', 'timesformer.encoder.layer.2.layernorm_after.bias', 'timesformer.encoder.layer.2.temporal_layernorm.weight', 'timesformer.encoder.layer.2.temporal_layernorm.bias', 'timesformer.encoder.layer.2.temporal_attention.attention.qkv.weight', 'timesformer.encoder.layer.2.temporal_attention.attention.qkv.bias', 'timesformer.encoder.layer.2.temporal_attention.output.dense.weight', 'timesformer.encoder.layer.2.temporal_attention.output.dense.bias', 'timesformer.encoder.layer.2.temporal_dense.weight', 'timesformer.encoder.layer.2.temporal_dense.bias', 'timesformer.encoder.layer.3.attention.attention.qkv.weight', 'timesformer.encoder.layer.3.attention.attention.qkv.bias', 'timesformer.encoder.layer.3.attention.output.dense.weight', 'timesformer.encoder.layer.3.attention.output.dense.bias', 'timesformer.encoder.layer.3.intermediate.dense.weight', 'timesformer.encoder.layer.3.intermediate.dense.bias', 'timesformer.encoder.layer.3.output.dense.weight', 'timesformer.encoder.layer.3.output.dense.bias', 'timesformer.encoder.layer.3.layernorm_before.weight', 'timesformer.encoder.layer.3.layernorm_before.bias', 'timesformer.encoder.layer.3.layernorm_after.weight', 'timesformer.encoder.layer.3.layernorm_after.bias', 'timesformer.encoder.layer.3.temporal_layernorm.weight', 'timesformer.encoder.layer.3.temporal_layernorm.bias', 'timesformer.encoder.layer.3.temporal_attention.attention.qkv.weight', 'timesformer.encoder.layer.3.temporal_attention.attention.qkv.bias', 'timesformer.encoder.layer.3.temporal_attention.output.dense.weight', 'timesformer.encoder.layer.3.temporal_attention.output.dense.bias', 'timesformer.encoder.layer.3.temporal_dense.weight', 'timesformer.encoder.layer.3.temporal_dense.bias', 'timesformer.encoder.layer.4.attention.attention.qkv.weight', 'timesformer.encoder.layer.4.attention.attention.qkv.bias', 'timesformer.encoder.layer.4.attention.output.dense.weight', 'timesformer.encoder.layer.4.attention.output.dense.bias', 'timesformer.encoder.layer.4.intermediate.dense.weight', 'timesformer.encoder.layer.4.intermediate.dense.bias', 'timesformer.encoder.layer.4.output.dense.weight', 'timesformer.encoder.layer.4.output.dense.bias', 'timesformer.encoder.layer.4.layernorm_before.weight', 'timesformer.encoder.layer.4.layernorm_before.bias', 'timesformer.encoder.layer.4.layernorm_after.weight', 'timesformer.encoder.layer.4.layernorm_after.bias', 'timesformer.encoder.layer.4.temporal_layernorm.weight', 'timesformer.encoder.layer.4.temporal_layernorm.bias', 'timesformer.encoder.layer.4.temporal_attention.attention.qkv.weight', 'timesformer.encoder.layer.4.temporal_attention.attention.qkv.bias', 'timesformer.encoder.layer.4.temporal_attention.output.dense.weight', 'timesformer.encoder.layer.4.temporal_attention.output.dense.bias', 'timesformer.encoder.layer.4.temporal_dense.weight', 'timesformer.encoder.layer.4.temporal_dense.bias', 'timesformer.encoder.layer.5.attention.attention.qkv.weight', 'timesformer.encoder.layer.5.attention.attention.qkv.bias', 'timesformer.encoder.layer.5.attention.output.dense.weight', 'timesformer.encoder.layer.5.attention.output.dense.bias', 'timesformer.encoder.layer.5.intermediate.dense.weight', 'timesformer.encoder.layer.5.intermediate.dense.bias', 'timesformer.encoder.layer.5.output.dense.weight', 'timesformer.encoder.layer.5.output.dense.bias', 'timesformer.encoder.layer.5.layernorm_before.weight', 'timesformer.encoder.layer.5.layernorm_before.bias', 'timesformer.encoder.layer.5.layernorm_after.weight', 'timesformer.encoder.layer.5.layernorm_after.bias', 'timesformer.encoder.layer.5.temporal_layernorm.weight', 'timesformer.encoder.layer.5.temporal_layernorm.bias', 'timesformer.encoder.layer.5.temporal_attention.attention.qkv.weight', 'timesformer.encoder.layer.5.temporal_attention.attention.qkv.bias', 'timesformer.encoder.layer.5.temporal_attention.output.dense.weight', 'timesformer.encoder.layer.5.temporal_attention.output.dense.bias', 'timesformer.encoder.layer.5.temporal_dense.weight', 'timesformer.encoder.layer.5.temporal_dense.bias', 'timesformer.encoder.layer.6.attention.attention.qkv.weight', 'timesformer.encoder.layer.6.attention.attention.qkv.bias', 'timesformer.encoder.layer.6.attention.output.dense.weight', 'timesformer.encoder.layer.6.attention.output.dense.bias', 'timesformer.encoder.layer.6.intermediate.dense.weight', 'timesformer.encoder.layer.6.intermediate.dense.bias', 'timesformer.encoder.layer.6.output.dense.weight', 'timesformer.encoder.layer.6.output.dense.bias', 'timesformer.encoder.layer.6.layernorm_before.weight', 'timesformer.encoder.layer.6.layernorm_before.bias', 'timesformer.encoder.layer.6.layernorm_after.weight', 'timesformer.encoder.layer.6.layernorm_after.bias', 'timesformer.encoder.layer.6.temporal_layernorm.weight', 'timesformer.encoder.layer.6.temporal_layernorm.bias', 'timesformer.encoder.layer.6.temporal_attention.attention.qkv.weight', 'timesformer.encoder.layer.6.temporal_attention.attention.qkv.bias', 'timesformer.encoder.layer.6.temporal_attention.output.dense.weight', 'timesformer.encoder.layer.6.temporal_attention.output.dense.bias', 'timesformer.encoder.layer.6.temporal_dense.weight', 'timesformer.encoder.layer.6.temporal_dense.bias', 'timesformer.encoder.layer.7.attention.attention.qkv.weight', 'timesformer.encoder.layer.7.attention.attention.qkv.bias', 'timesformer.encoder.layer.7.attention.output.dense.weight', 'timesformer.encoder.layer.7.attention.output.dense.bias', 'timesformer.encoder.layer.7.intermediate.dense.weight', 'timesformer.encoder.layer.7.intermediate.dense.bias', 'timesformer.encoder.layer.7.output.dense.weight', 'timesformer.encoder.layer.7.output.dense.bias', 'timesformer.encoder.layer.7.layernorm_before.weight', 'timesformer.encoder.layer.7.layernorm_before.bias', 'timesformer.encoder.layer.7.layernorm_after.weight', 'timesformer.encoder.layer.7.layernorm_after.bias', 'timesformer.encoder.layer.7.temporal_layernorm.weight', 'timesformer.encoder.layer.7.temporal_layernorm.bias', 'timesformer.encoder.layer.7.temporal_attention.attention.qkv.weight', 'timesformer.encoder.layer.7.temporal_attention.attention.qkv.bias', 'timesformer.encoder.layer.7.temporal_attention.output.dense.weight', 'timesformer.encoder.layer.7.temporal_attention.output.dense.bias', 'timesformer.encoder.layer.7.temporal_dense.weight', 'timesformer.encoder.layer.7.temporal_dense.bias', 'timesformer.encoder.layer.8.attention.attention.qkv.weight', 'timesformer.encoder.layer.8.attention.attention.qkv.bias', 'timesformer.encoder.layer.8.attention.output.dense.weight', 'timesformer.encoder.layer.8.attention.output.dense.bias', 'timesformer.encoder.layer.8.intermediate.dense.weight', 'timesformer.encoder.layer.8.intermediate.dense.bias', 'timesformer.encoder.layer.8.output.dense.weight', 'timesformer.encoder.layer.8.output.dense.bias', 'timesformer.encoder.layer.8.layernorm_before.weight', 'timesformer.encoder.layer.8.layernorm_before.bias', 'timesformer.encoder.layer.8.layernorm_after.weight', 'timesformer.encoder.layer.8.layernorm_after.bias', 'timesformer.encoder.layer.8.temporal_layernorm.weight', 'timesformer.encoder.layer.8.temporal_layernorm.bias', 'timesformer.encoder.layer.8.temporal_attention.attention.qkv.weight', 'timesformer.encoder.layer.8.temporal_attention.attention.qkv.bias', 'timesformer.encoder.layer.8.temporal_attention.output.dense.weight', 'timesformer.encoder.layer.8.temporal_attention.output.dense.bias', 'timesformer.encoder.layer.8.temporal_dense.weight', 'timesformer.encoder.layer.8.temporal_dense.bias', 'timesformer.encoder.layer.9.attention.attention.qkv.weight', 'timesformer.encoder.layer.9.attention.attention.qkv.bias', 'timesformer.encoder.layer.9.attention.output.dense.weight', 'timesformer.encoder.layer.9.attention.output.dense.bias', 'timesformer.encoder.layer.9.intermediate.dense.weight', 'timesformer.encoder.layer.9.intermediate.dense.bias', 'timesformer.encoder.layer.9.output.dense.weight', 'timesformer.encoder.layer.9.output.dense.bias', 'timesformer.encoder.layer.9.layernorm_before.weight', 'timesformer.encoder.layer.9.layernorm_before.bias', 'timesformer.encoder.layer.9.layernorm_after.weight', 'timesformer.encoder.layer.9.layernorm_after.bias', 'timesformer.encoder.layer.9.temporal_layernorm.weight', 'timesformer.encoder.layer.9.temporal_layernorm.bias', 'timesformer.encoder.layer.9.temporal_attention.attention.qkv.weight', 'timesformer.encoder.layer.9.temporal_attention.attention.qkv.bias', 'timesformer.encoder.layer.9.temporal_attention.output.dense.weight', 'timesformer.encoder.layer.9.temporal_attention.output.dense.bias', 'timesformer.encoder.layer.9.temporal_dense.weight', 'timesformer.encoder.layer.9.temporal_dense.bias', 'timesformer.encoder.layer.10.attention.attention.qkv.weight', 'timesformer.encoder.layer.10.attention.attention.qkv.bias', 'timesformer.encoder.layer.10.attention.output.dense.weight', 'timesformer.encoder.layer.10.attention.output.dense.bias', 'timesformer.encoder.layer.10.intermediate.dense.weight', 'timesformer.encoder.layer.10.intermediate.dense.bias', 'timesformer.encoder.layer.10.output.dense.weight', 'timesformer.encoder.layer.10.output.dense.bias', 'timesformer.encoder.layer.10.layernorm_before.weight', 'timesformer.encoder.layer.10.layernorm_before.bias', 'timesformer.encoder.layer.10.layernorm_after.weight', 'timesformer.encoder.layer.10.layernorm_after.bias', 'timesformer.encoder.layer.10.temporal_layernorm.weight', 'timesformer.encoder.layer.10.temporal_layernorm.bias', 'timesformer.encoder.layer.10.temporal_attention.attention.qkv.weight', 'timesformer.encoder.layer.10.temporal_attention.attention.qkv.bias', 'timesformer.encoder.layer.10.temporal_attention.output.dense.weight', 'timesformer.encoder.layer.10.temporal_attention.output.dense.bias', 'timesformer.encoder.layer.10.temporal_dense.weight', 'timesformer.encoder.layer.10.temporal_dense.bias', 'timesformer.encoder.layer.11.attention.attention.qkv.weight', 'timesformer.encoder.layer.11.attention.attention.qkv.bias', 'timesformer.encoder.layer.11.attention.output.dense.weight', 'timesformer.encoder.layer.11.attention.output.dense.bias', 'timesformer.encoder.layer.11.intermediate.dense.weight', 'timesformer.encoder.layer.11.intermediate.dense.bias', 'timesformer.encoder.layer.11.output.dense.weight', 'timesformer.encoder.layer.11.output.dense.bias', 'timesformer.encoder.layer.11.layernorm_before.weight', 'timesformer.encoder.layer.11.layernorm_before.bias', 'timesformer.encoder.layer.11.layernorm_after.weight', 'timesformer.encoder.layer.11.layernorm_after.bias', 'timesformer.encoder.layer.11.temporal_layernorm.weight', 'timesformer.encoder.layer.11.temporal_layernorm.bias', 'timesformer.encoder.layer.11.temporal_attention.attention.qkv.weight', 'timesformer.encoder.layer.11.temporal_attention.attention.qkv.bias', 'timesformer.encoder.layer.11.temporal_attention.output.dense.weight', 'timesformer.encoder.layer.11.temporal_attention.output.dense.bias', 'timesformer.encoder.layer.11.temporal_dense.weight', 'timesformer.encoder.layer.11.temporal_dense.bias', 'timesformer.layernorm.weight', 'timesformer.layernorm.bias', 'classifier.weight', 'classifier.bias'])\n",
            "Accuracy: 0.5319\n",
            "Precision: 0.4845\n",
            "Recall: 0.5319\n",
            "F1-Score: 0.4926\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoConfig, AutoModelForVideoClassification\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Function to load the model\n",
        "def load_model(checkpoint_path, config_path=\"facebook/timesformer-base-finetuned-k400\"):\n",
        "    # Load the configuration\n",
        "    config = AutoConfig.from_pretrained(config_path)\n",
        "    config.image_size = 64  # Ensure the image size matches your fine-tuned model's setting\n",
        "    config.num_labels = 14  # Number of output classes\n",
        "    config.patch_size = 8  # Patch size\n",
        "\n",
        "    # Load the model\n",
        "    model = AutoModelForVideoClassification.from_pretrained(\n",
        "        config_path, \n",
        "        config=config, \n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "\n",
        "    # Load the checkpoint (fine-tuned weights)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))\n",
        "\n",
        "    # Debug: Print available keys in the checkpoint\n",
        "    print(\"Checkpoint Keys:\", checkpoint.keys())\n",
        "\n",
        "    # Load model weights (handling different save formats)\n",
        "    if \"model_state_dict\" in checkpoint:  # If saved with torch.save(model.state_dict(), path)\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    else:  # If saved with torch.save(model, path)\n",
        "        model.load_state_dict(checkpoint)\n",
        "\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    return model\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    model.to(device)  # Move model to GPU/CPU\n",
        "\n",
        "    # Iterate over the test dataset\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            frames, labels = batch\n",
        "            frames = frames.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass: Get model predictions\n",
        "            outputs = model(frames)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=1)  # Get the class with the highest probability\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate accuracy, precision, recall, and F1 score\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "    \n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plot_confusion_matrix(cm, labels=list(range(14)))  # Adjust if you have class names\n",
        "\n",
        "    return accuracy, precision, recall, f1, cm\n",
        "\n",
        "# Function to plot the confusion matrix\n",
        "def plot_confusion_matrix(cm, labels):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('True labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Path to the fine-tuned model checkpoint\n",
        "checkpoint_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\best_model_epoch_12.pth\"\n",
        "\n",
        "# Device selection\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model(checkpoint_path)\n",
        "model.to(device)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy, precision, recall, f1, cm = evaluate_model(model, test_loader, device)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "019243e7224f4e248b46135e2193a267": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08390d94f7984820a4f3081bab293101": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a99f2603c0241488694a0ba3a7f395b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f445d2cbe164eac9943b9169e05a066": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b87464a1460d4ff4b21b428fbeaf7aae",
            "max": 486348721,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73d355f2b37b40e0a7e61dbccc3c436f",
            "value": 486348721
          }
        },
        "2212e87e4fa246bfb80fc3de5709a792": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22cf733bc0524ac4ab466e0089c03cea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31d21d02b8ce43acb2968082191ec8ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "333ec5cd772843fe8fc041eca37e8d89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "398196f92fe3458dbbd5c91a2aa563fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a95b4b48428482584e5e93998ec294d",
              "IPY_MODEL_e9bf0034dea1412aafafed56e81bf084",
              "IPY_MODEL_99bcf45f49ea465280fd15430b317439"
            ],
            "layout": "IPY_MODEL_8de70ed3a3ec4ef2b3ebf29112a990c9"
          }
        },
        "44889dc130534b3dae0704d39094959c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a95b4b48428482584e5e93998ec294d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2212e87e4fa246bfb80fc3de5709a792",
            "placeholder": "​",
            "style": "IPY_MODEL_80260a16422c4ff8962bb836552dac13",
            "value": "config.json: 100%"
          }
        },
        "543ded25c4b04a1bae801316b41e7b14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae1a06c80fab4bc29fa587c4c128718c",
            "placeholder": "​",
            "style": "IPY_MODEL_a7fa1c194d4048a388e84c8c3342de29",
            "value": " 412/412 [00:00&lt;00:00, 9.75kB/s]"
          }
        },
        "5fbf44d3fb1e468986d7e1c2b64a3f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73d355f2b37b40e0a7e61dbccc3c436f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7839e369c8b5439d82a3ede210e93556": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80260a16422c4ff8962bb836552dac13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8758105677f24949ad39c3aa650a165a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8de70ed3a3ec4ef2b3ebf29112a990c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99bcf45f49ea465280fd15430b317439": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08390d94f7984820a4f3081bab293101",
            "placeholder": "​",
            "style": "IPY_MODEL_9e348447ac2d431aa4f20d65c7483a0c",
            "value": " 22.7k/22.7k [00:00&lt;00:00, 415kB/s]"
          }
        },
        "9e348447ac2d431aa4f20d65c7483a0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e446983144145f38f3fe6a8696f7e74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22cf733bc0524ac4ab466e0089c03cea",
            "placeholder": "​",
            "style": "IPY_MODEL_7839e369c8b5439d82a3ede210e93556",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "a70404df53a544b4a53fe4a9d0d36c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e446983144145f38f3fe6a8696f7e74",
              "IPY_MODEL_1f445d2cbe164eac9943b9169e05a066",
              "IPY_MODEL_ffeac10b9ea34e66bc5b68302529eafc"
            ],
            "layout": "IPY_MODEL_333ec5cd772843fe8fc041eca37e8d89"
          }
        },
        "a7fa1c194d4048a388e84c8c3342de29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a99f77b726cc4a4a9fd50a3e51960fa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab0e40a656df47c9a7eeb3b661c30a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1b317ee181c40c8a6747c42f7b7b93b",
              "IPY_MODEL_b705ae74ea354e45980fd45fe04f2ce0",
              "IPY_MODEL_543ded25c4b04a1bae801316b41e7b14"
            ],
            "layout": "IPY_MODEL_1a99f2603c0241488694a0ba3a7f395b"
          }
        },
        "ae1a06c80fab4bc29fa587c4c128718c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1b317ee181c40c8a6747c42f7b7b93b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_019243e7224f4e248b46135e2193a267",
            "placeholder": "​",
            "style": "IPY_MODEL_8758105677f24949ad39c3aa650a165a",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "b705ae74ea354e45980fd45fe04f2ce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44889dc130534b3dae0704d39094959c",
            "max": 412,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31d21d02b8ce43acb2968082191ec8ce",
            "value": 412
          }
        },
        "b87464a1460d4ff4b21b428fbeaf7aae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3d3b5b0473b4243a781da67a0e7bc32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3322e17cba64302a6fd52c6ddd7b462": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9bf0034dea1412aafafed56e81bf084": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fbf44d3fb1e468986d7e1c2b64a3f7c",
            "max": 22723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a99f77b726cc4a4a9fd50a3e51960fa9",
            "value": 22723
          }
        },
        "ffeac10b9ea34e66bc5b68302529eafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3322e17cba64302a6fd52c6ddd7b462",
            "placeholder": "​",
            "style": "IPY_MODEL_c3d3b5b0473b4243a781da67a0e7bc32",
            "value": " 486M/486M [00:05&lt;00:00, 154MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
