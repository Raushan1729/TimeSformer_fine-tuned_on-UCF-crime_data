{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TimeSformer fine-tuned on UCF Crime dataset available on kaggle with 64*64 pixel size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Downloading the data from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQC6YTGMOkux"
      },
      "outputs": [],
      "source": [
        "pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "5XPbSs_oOx8h",
        "outputId": "626c6c23-c76b-48c0-f9fa-0e1915e2ba55"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9KKS6flPI2-"
      },
      "outputs": [],
      "source": [
        " ! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Fz2NHlQPNmE"
      },
      "outputs": [],
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRIw7wGPPWQ9"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQCDMofNPdCb",
        "outputId": "68dc06c1-77ac-41c7-8ab3-7d8b59943026"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSXcChpUPpxA",
        "outputId": "1a60891e-d103-465a-b934-0831de011e74"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d odins0n/ucf-crime-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unziping the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBCARKVrRR6O",
        "outputId": "c1a8cb68-9fa8-4b60-e7ca-33a10195fbab"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "zip_path= \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\ucf-crime-dataset.zip\"\n",
        "\n",
        "with ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('C:\\\\Users\\\\user\\\\Desktop\\\\raushan')\n",
        "print('unzipping completed')   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the dataset from directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeA2AG3BUCo0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define a function to collect all frame paths and their respective labels\n",
        "def load_dataset(root_dir):\n",
        "    classes = os.listdir(root_dir)\n",
        "    frame_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for class_idx, class_name in enumerate(classes):\n",
        "        class_path = os.path.join(root_dir, class_name)\n",
        "        images = glob(os.path.join(class_path, \"*.png\"))\n",
        "        frame_paths.extend(images)\n",
        "        labels.extend([class_idx] * len(images))\n",
        "\n",
        "    return frame_paths, labels\n",
        "\n",
        "# Load train and test datasets\n",
        "train_root_dir = \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\Train\"\n",
        "test_root_dir = \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\Test\"\n",
        "\n",
        "train_frame_paths, train_labels = load_dataset(train_root_dir)\n",
        "test_frame_paths, test_labels = load_dataset(test_root_dir)\n",
        "\n",
        "# Optionally split the train data for validation\n",
        "train_frame_paths, val_frame_paths, train_labels, val_labels = train_test_split(\n",
        "    train_frame_paths, train_labels, test_size=0.1, random_state=42,train_size=0.9\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYxLw9MBVCsL",
        "outputId": "2f007598-34b0-43c3-8928-8a46f53da98d"
      },
      "outputs": [],
      "source": [
        "print(len(train_frame_paths))\n",
        "print(len(val_frame_paths))\n",
        "print(len(test_frame_paths))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing Data Set Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5AWeI4ElrAF",
        "outputId": "84929799-e3cf-432e-ec3e-84be5bc2d28f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from glob import glob\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define a custom dataset to handle the UCF Crime frames and group them into sequences\n",
        "class FrameSequenceDataset(Dataset):\n",
        "    def __init__(self, root_dir, frame_count=16, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.frame_count = frame_count\n",
        "        self.transform = transform\n",
        "        self.classes = os.listdir(root_dir)\n",
        "        self.frame_sequences = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Debug: Print the classes found\n",
        "        print(\"Classes found:\", self.classes)\n",
        "\n",
        "        # Iterate over each class and gather frame sequences\n",
        "        for class_idx, class_name in enumerate(self.classes):\n",
        "            class_path = os.path.join(root_dir, class_name)\n",
        "            frames = sorted(glob(os.path.join(class_path, \"*.png\")))\n",
        "            print(f\"Found {len(frames)} frames in {class_name}\")  # Debug: Check frames in each class\n",
        "\n",
        "            # Group frames into sequences of self.frame_count\n",
        "            for i in range(0, len(frames), self.frame_count):\n",
        "                frame_seq = frames[i:i + self.frame_count]\n",
        "                if len(frame_seq) == self.frame_count:\n",
        "                    self.frame_sequences.append(frame_seq)\n",
        "                    self.labels.append(class_idx)\n",
        "\n",
        "        # Debug: Print the number of sequences created\n",
        "        print(\"Total sequences created:\", len(self.frame_sequences))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frame_sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        frame_seq_paths = self.frame_sequences[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load and preprocess frames\n",
        "        frames = [Image.open(frame_path).convert(\"RGB\") for frame_path in frame_seq_paths]\n",
        "        if self.transform:\n",
        "            frames = [self.transform(frame) for frame in frames]\n",
        "\n",
        "        # Stack the frames (TimeSformer expects them as a tensor)\n",
        "        frames = torch.stack(frames)\n",
        "\n",
        "        return frames, torch.tensor(label)\n",
        "\n",
        "# Image transforms (resize to 224x224 for TimeSformer)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    #transforms.RandomHoizontalFlip(),\n",
        "    transforms.ColorJitter(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load the train dataset\n",
        "train_dataset = FrameSequenceDataset(root_dir=\"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\Train\", frame_count=16, transform=transform)\n",
        "#val_dataset = FrameSequenceDataset(root_dir=\"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\val\", frame_count=16, transform=transform)  # Remove if not needed\n",
        "test_dataset = FrameSequenceDataset(root_dir=\"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\Test\", frame_count=16, transform=transform)\n",
        "\n",
        "# DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "#val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)  # Remove if not needed\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Check the lengths of the datasets\n",
        "print(\"Train dataset length:\", len(train_dataset))\n",
        "# print(\"Validation dataset length:\", len(val_dataset))  # Remove if not needed\n",
        "print(\"Test dataset length:\", len(test_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adjusting model configration according to the data available so that it can learn maximum of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoModelForVideoClassification\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
        "config.image_size = 64\n",
        "config.num_labels = 14\n",
        "config.patch_size = 8\n",
        "\n",
        "model = AutoModelForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\", config =config, ignore_mismatched_sizes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "ab0e40a656df47c9a7eeb3b661c30a9e",
            "b1b317ee181c40c8a6747c42f7b7b93b",
            "b705ae74ea354e45980fd45fe04f2ce0",
            "543ded25c4b04a1bae801316b41e7b14",
            "1a99f2603c0241488694a0ba3a7f395b",
            "019243e7224f4e248b46135e2193a267",
            "8758105677f24949ad39c3aa650a165a",
            "44889dc130534b3dae0704d39094959c",
            "31d21d02b8ce43acb2968082191ec8ce",
            "ae1a06c80fab4bc29fa587c4c128718c",
            "a7fa1c194d4048a388e84c8c3342de29",
            "398196f92fe3458dbbd5c91a2aa563fc",
            "4a95b4b48428482584e5e93998ec294d",
            "e9bf0034dea1412aafafed56e81bf084",
            "99bcf45f49ea465280fd15430b317439",
            "8de70ed3a3ec4ef2b3ebf29112a990c9",
            "2212e87e4fa246bfb80fc3de5709a792",
            "80260a16422c4ff8962bb836552dac13",
            "5fbf44d3fb1e468986d7e1c2b64a3f7c",
            "a99f77b726cc4a4a9fd50a3e51960fa9",
            "08390d94f7984820a4f3081bab293101",
            "9e348447ac2d431aa4f20d65c7483a0c",
            "a70404df53a544b4a53fe4a9d0d36c48",
            "9e446983144145f38f3fe6a8696f7e74",
            "1f445d2cbe164eac9943b9169e05a066",
            "ffeac10b9ea34e66bc5b68302529eafc",
            "333ec5cd772843fe8fc041eca37e8d89",
            "22cf733bc0524ac4ab466e0089c03cea",
            "7839e369c8b5439d82a3ede210e93556",
            "b87464a1460d4ff4b21b428fbeaf7aae",
            "73d355f2b37b40e0a7e61dbccc3c436f",
            "d3322e17cba64302a6fd52c6ddd7b462",
            "c3d3b5b0473b4243a781da67a0e7bc32"
          ]
        },
        "id": "K4Kl667PpKls",
        "outputId": "70a8ffb1-f994-4064-adce-932641104e26"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoImageProcessor, AutoModelForVideoClassification\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
        "#model = AutoModelForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\",\n",
        "                                                      # num_labels=len(train_dataset.classes), # This will correctly set the output to 14 classes\n",
        "                                                      # ignore_mismatched_sizes=True) # This will handle the size mismatch in the classifier layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parametrs you want to learn. Eg if you want to freeze the parametres and only want to train the last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5vpeZjcmhxC"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "import torch\n",
        "\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "'''\n",
        "# Freeze the first few layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the final classification head for fine-tuning\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
        "'''\n",
        "# Define optimizer and learning rate\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "OgiZLOoyrULS",
        "outputId": "60b1750c-9fa6-4569-d3cf-6c961277865f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Initialize TensorBoard for visualizing the training and validation loss\n",
        "writer = SummaryWriter(log_dir=\"runs/experiment_1\")\n",
        "\n",
        "# Path where you want to save the model\n",
        "save_dir = \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\\"  # Your desired path\n",
        "\n",
        "# Make sure the save directory exists\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # Increased number of epochs for better training\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "# Ensure model is on the right device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training and validation loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
        "        frames, labels = batch\n",
        "        frames = frames.to(device)  # Move frames to GPU/CPU\n",
        "        labels = labels.to(device)  # Move labels to GPU/CPU\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(frames, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Log the training loss to TensorBoard\n",
        "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
        "            frames, labels = batch\n",
        "            frames = frames.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(frames, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(test_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Log the validation loss to TensorBoard\n",
        "    writer.add_scalar('Loss/validation', avg_val_loss, epoch)\n",
        "\n",
        "    # Save the best model based on validation loss\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        \n",
        "        # Save the model, optimizer, and epoch to a checkpoint\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_val_loss,\n",
        "        }\n",
        "        checkpoint_path = os.path.join(save_dir, f\"best_model_epoch_{epoch+1}.pth\")\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        print(f\"Best model saved at epoch {epoch+1} to {checkpoint_path}\")\n",
        "\n",
        "# Close the TensorBoard writer\n",
        "writer.close()\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoModelForVideoClassification, AutoConfig\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Check device (GPU/CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load model configuration with 14 output classes\n",
        "#config = AutoConfig.from_pretrained(\"facebook/timesformer-base-finetuned-k400\", num_labels=14)\n",
        "\n",
        "# Load pre-trained model, handling shape mismatches\n",
        "model = AutoModelForVideoClassification.from_pretrained(\n",
        "    \"facebook/timesformer-base-finetuned-k400\",\n",
        "    config=config,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Replace classifier with a new one for 14 classes\n",
        "model.classifier = nn.Linear(model.config.hidden_size, 14)\n",
        "model.to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Define dataset (replace with actual dataset loading)\n",
        "# Example: train_dataset should be a PyTorch dataset of video frames and labels\n",
        "# train_dataset = YourCustomDataset()\n",
        "total_samples = len(train_dataset)\n",
        "val_split = int(0.2 * total_samples)  # 20% validation\n",
        "train_split = total_samples - val_split\n",
        "\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_split, val_split])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Output directory for saving models\n",
        "save_dir = \"C://Users//user//Desktop//raushan/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs} - Training:\")\n",
        "    train_bar = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch}/{num_epochs} - Training\")\n",
        "    \n",
        "    for frames, labels in train_bar:\n",
        "        frames, labels = frames.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(frames).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        train_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs} - Validation:\")\n",
        "    val_bar = tqdm(val_loader, total=len(val_loader), desc=f\"Epoch {epoch}/{num_epochs} - Validation\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for frames, labels in val_bar:\n",
        "            frames, labels = frames.to(device), labels.to(device)\n",
        "            outputs = model(frames).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            val_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}], Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_model_path = os.path.join(save_dir, f\"best_model_epoch_{epoch}.pth\")\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Best model saved at epoch {epoch} to {best_model_path}\")\n",
        "\n",
        "    # Save last epoch model\n",
        "    last_model_path = os.path.join(save_dir, \"last_epoch_model.pth\")\n",
        "    torch.save(model.state_dict(), last_model_path)\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()  # Clears unused memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training from particular checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import AutoModelForVideoClassification\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Check device (GPU/CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Path to the saved checkpoint (from epoch 10)\n",
        "checkpoint_path = \"C://Users//user//Desktop//raushan//best_model_epoch_13.pth\"\n",
        "\n",
        "# Load pre-trained model\n",
        "model = AutoModelForVideoClassification.from_pretrained(\n",
        "    \"facebook/timesformer-base-finetuned-k400\",\n",
        "    config=config,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "\n",
        "# Replace classifier with a new one for 14 classes (ensuring correct output shape)\n",
        "model.classifier = nn.Linear(model.config.hidden_size, 14)\n",
        "\n",
        "# Load the saved checkpoint\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "\n",
        "# Move model to the appropriate device\n",
        "model.to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Define dataset (ensure `train_dataset` is properly defined)\n",
        "total_samples = len(train_dataset)\n",
        "val_split = int(0.2 * total_samples)  # 20% validation\n",
        "train_split = total_samples - val_split\n",
        "\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_split, val_split])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Output directory for saving models\n",
        "save_dir = \"C://Users//user//Desktop//raushan/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Resume training from epoch 11\n",
        "start_epoch = 15\n",
        "num_epochs = 20\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs} - Training:\")\n",
        "    train_bar = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch}/{num_epochs} - Training\")\n",
        "\n",
        "    for frames, labels in train_bar:\n",
        "        frames, labels = frames.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(frames).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        train_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs} - Validation:\")\n",
        "    val_bar = tqdm(val_loader, total=len(val_loader), desc=f\"Epoch {epoch}/{num_epochs} - Validation\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for frames, labels in val_bar:\n",
        "            frames, labels = frames.to(device), labels.to(device)\n",
        "            outputs = model(frames).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            val_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}], Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_model_path = os.path.join(save_dir, f\"best_model_epoch_{epoch}.pth\")\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Best model saved at epoch {epoch} to {best_model_path}\")\n",
        "\n",
        "    # Save last epoch model\n",
        "    last_model_path = os.path.join(save_dir, \"last_epoch_model.pth\")\n",
        "    torch.save(model.state_dict(), last_model_path)\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.set_per_process_memory_fraction(0.9, device=0)  # Allocates 90% of GPU memory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Metrics of finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import AutoModelForVideoClassification, AutoConfig\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load model configuration\n",
        "config = AutoConfig.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
        "config.image_size = 64  # Set same as training\n",
        "config.num_labels = 14  # Number of classes\n",
        "config.patch_size = 8\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForVideoClassification.from_pretrained(\n",
        "    \"facebook/timesformer-base-finetuned-k400\",\n",
        "    config=config,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Load trained weights\n",
        "model_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\best_model_epoch_17.pth\"  # Change if using best model\n",
        "assert os.path.exists(model_path), f\"Model file not found: {model_path}\"\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define label mapping from dataset\n",
        "class_labels = os.listdir(\"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\Train\")  # Same as used in training\n",
        "print(\"Class labels:\", class_labels)\n",
        "\n",
        "# Initialize lists to store predictions and ground truths\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Evaluate on test dataset\n",
        "print(\"\\nEvaluating on Test Dataset...\\n\")\n",
        "with torch.no_grad():\n",
        "    for frames, labels in tqdm(test_loader, total=len(test_loader), desc=\"Testing\"):\n",
        "        frames, labels = frames.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(frames).logits  # Get model predictions\n",
        "        preds = torch.argmax(outputs, dim=1)  # Get predicted class indices\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert indices to class names\n",
        "predicted_labels = [class_labels[i] for i in all_preds]\n",
        "actual_labels = [class_labels[i] for i in all_labels]\n",
        "\n",
        "# Compute accuracy, precision, recall, and F1-score\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"weighted\")\n",
        "\n",
        "# Print results\n",
        "print(\"\\nTest Performance Metrics:\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "# Print a few predictions for reference\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(10):  # Print 10 sample predictions\n",
        "    if i < len(predicted_labels):\n",
        "        print(f\"Actual: {actual_labels[i]}  |  Predicted: {predicted_labels[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predict the class of your video with the finetuned model. Enter the model path where the checkpoints have been saved and execute the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "from transformers import AutoModelForVideoClassification, AutoConfig\n",
        "\n",
        "# Set device (Use GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define class labels (Update this with your actual labels)\n",
        "CLASS_LABELS = {\n",
        "    0: \"Abuse\",\n",
        "    1: \"Arrest\",\n",
        "    2: \"Arson\",\n",
        "    3: \"Assault\",\n",
        "    4: \"Burglary\",\n",
        "    5: \"Explosion\",\n",
        "    6: \"Fighting\",\n",
        "    7: \"Normalvideos\",\n",
        "    8: \"RoadAccidents\",\n",
        "    9: \"Robbery\",\n",
        "    10: \"Shooting\",\n",
        "    11: \"Shoplifting\",\n",
        "    12: \"Stealing\",\n",
        "    13: \"Vandalism\"  # Example: Update these labels accordingly\n",
        "}\n",
        "\n",
        "# Load trained model function\n",
        "def load_trained_model(model_path):\n",
        "    # Load model config\n",
        "    config = AutoConfig.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
        "    config.image_size = 64  # Ensure it matches training config\n",
        "    config.num_labels = 14\n",
        "    config.patch_size = 8\n",
        "\n",
        "    # Load model with modified config\n",
        "    model = AutoModelForVideoClassification.from_pretrained(\n",
        "        \"facebook/timesformer-base-finetuned-k400\",\n",
        "        config=config,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "    \n",
        "    # Replace classifier with 14-class output\n",
        "    model.classifier = torch.nn.Linear(model.config.hidden_size, 14)\n",
        "    \n",
        "    # Load trained weights\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "# Function to extract frames from a video\n",
        "def extract_frames(video_path, num_frames=8, frame_size=(224, 224)):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    if total_frames == 0:\n",
        "        print(\"Error: Cannot read video.\")\n",
        "        return None\n",
        "\n",
        "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "    frames = []\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize(frame_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225])\n",
        "    ])\n",
        "\n",
        "    for i in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            continue\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = transform(frame)\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) < num_frames:\n",
        "        print(\"Error: Not enough frames extracted.\")\n",
        "        return None  # Return None if not enough frames are available\n",
        "\n",
        "    return torch.stack(frames)  # Shape: (num_frames, 3, 224, 224)\n",
        "\n",
        "# Function to predict class from a video\n",
        "def predict_on_video(video_path, model):\n",
        "    frames = extract_frames(video_path)\n",
        "    if frames is None:\n",
        "        return None\n",
        "\n",
        "    frames = frames.unsqueeze(0).to(device)  # Add batch dimension: (1, num_frames, 3, 224, 224)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(frames).logits  # Get model predictions\n",
        "        predicted_class_idx = torch.argmax(outputs, dim=1).item()  # Get predicted class index\n",
        "\n",
        "    # Get class label from dictionary\n",
        "    predicted_label = CLASS_LABELS.get(predicted_class_idx, \"Unknown\")\n",
        "\n",
        "    return predicted_class_idx, predicted_label\n",
        "\n",
        "# Load trained model\n",
        "model_path = \"C:\\\\Users\\\\user\\Desktop\\\\raushan\\\\best_model_epoch_12.pth\"  # Change this path\n",
        "model = load_trained_model(model_path)\n",
        "\n",
        "# Test on a video\n",
        "video_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\mixkit-strong-female-mixed-martial-arts-fighter-40991-hd-ready.mp4\"  # Change this path\n",
        "pred_class_idx, predicted_label = predict_on_video(video_path, model)\n",
        "\n",
        "if predicted_label is not None:\n",
        "    print(f\"Predicted Class Index: {pred_class_idx}\")\n",
        "    print(f\"Predicted Label: {predicted_label}\")\n",
        "else:\n",
        "    print(\"Prediction failed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the trained model\n",
        "def load_trained_model(model_path=\"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\best_model_epoch_10.pth\"):\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    print(\"Model loaded successfully!\")\n",
        "    return model\n",
        "\n",
        "# Read video frames using OpenCV\n",
        "def read_video_opencv(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    \n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((64, 64)),  # Resize frames to match model input\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    \n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "        frames.append(transform(frame))\n",
        "    \n",
        "    cap.release()\n",
        "    \n",
        "    if len(frames) == 0:\n",
        "        raise ValueError(\"No frames were extracted from the video!\")\n",
        "    \n",
        "    return torch.stack(frames)\n",
        "\n",
        "# Predict the class of a video\n",
        "def predict_on_video(video_path, model):\n",
        "    frames = read_video_opencv(video_path)  # Extract frames\n",
        "    frames = frames.unsqueeze(0).to(device)  # Add batch dimension\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(frames).logits  # Get model output\n",
        "        pred_class = torch.argmax(outputs, dim=1).item()  # Get predicted class\n",
        "    \n",
        "    return pred_class\n",
        "\n",
        "# Example usage\n",
        "model = load_trained_model(\"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\best_model_epoch_10.pth\")  # Load trained model\n",
        "video_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\videoplayback (online-video-cutter.com).mp4\"  # Change this to your video file path\n",
        "predicted_class = predict_on_video(video_path, model)\n",
        "print(f\"Predicted Class: {predicted_class}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation of the fine-tuned model and confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoConfig, AutoModelForVideoClassification\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Function to load the model\n",
        "def load_model(checkpoint_path, config_path=\"facebook/timesformer-base-finetuned-k400\"):\n",
        "    # Load the configuration\n",
        "    config = AutoConfig.from_pretrained(config_path)\n",
        "    config.image_size = 64  # Ensure the image size matches your fine-tuned model's setting\n",
        "    config.num_labels = 14  # Number of output classes\n",
        "    config.patch_size = 8  # Patch size\n",
        "\n",
        "    # Load the model\n",
        "    model = AutoModelForVideoClassification.from_pretrained(\n",
        "        config_path, \n",
        "        config=config, \n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "\n",
        "    # Load the checkpoint (fine-tuned weights)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))\n",
        "\n",
        "    # Debug: Print available keys in the checkpoint\n",
        "    print(\"Checkpoint Keys:\", checkpoint.keys())\n",
        "\n",
        "    # Load model weights (handling different save formats)\n",
        "    if \"model_state_dict\" in checkpoint:  # If saved with torch.save(model.state_dict(), path)\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    else:  # If saved with torch.save(model, path)\n",
        "        model.load_state_dict(checkpoint)\n",
        "\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    return model\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    model.to(device)  # Move model to GPU/CPU\n",
        "\n",
        "    # Iterate over the test dataset\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            frames, labels = batch\n",
        "            frames = frames.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass: Get model predictions\n",
        "            outputs = model(frames)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=1)  # Get the class with the highest probability\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate accuracy, precision, recall, and F1 score\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "    \n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plot_confusion_matrix(cm, labels=list(range(14)))  # Adjust if you have class names\n",
        "\n",
        "    return accuracy, precision, recall, f1, cm\n",
        "\n",
        "# Function to plot the confusion matrix\n",
        "def plot_confusion_matrix(cm, labels):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('True labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Path to the fine-tuned model checkpoint\n",
        "checkpoint_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\raushan\\\\best_model_epoch_12.pth\"\n",
        "\n",
        "# Device selection\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model(checkpoint_path)\n",
        "model.to(device)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy, precision, recall, f1, cm = evaluate_model(model, test_loader, device)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "019243e7224f4e248b46135e2193a267": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08390d94f7984820a4f3081bab293101": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a99f2603c0241488694a0ba3a7f395b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f445d2cbe164eac9943b9169e05a066": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b87464a1460d4ff4b21b428fbeaf7aae",
            "max": 486348721,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73d355f2b37b40e0a7e61dbccc3c436f",
            "value": 486348721
          }
        },
        "2212e87e4fa246bfb80fc3de5709a792": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22cf733bc0524ac4ab466e0089c03cea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31d21d02b8ce43acb2968082191ec8ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "333ec5cd772843fe8fc041eca37e8d89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "398196f92fe3458dbbd5c91a2aa563fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a95b4b48428482584e5e93998ec294d",
              "IPY_MODEL_e9bf0034dea1412aafafed56e81bf084",
              "IPY_MODEL_99bcf45f49ea465280fd15430b317439"
            ],
            "layout": "IPY_MODEL_8de70ed3a3ec4ef2b3ebf29112a990c9"
          }
        },
        "44889dc130534b3dae0704d39094959c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a95b4b48428482584e5e93998ec294d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2212e87e4fa246bfb80fc3de5709a792",
            "placeholder": "​",
            "style": "IPY_MODEL_80260a16422c4ff8962bb836552dac13",
            "value": "config.json: 100%"
          }
        },
        "543ded25c4b04a1bae801316b41e7b14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae1a06c80fab4bc29fa587c4c128718c",
            "placeholder": "​",
            "style": "IPY_MODEL_a7fa1c194d4048a388e84c8c3342de29",
            "value": " 412/412 [00:00&lt;00:00, 9.75kB/s]"
          }
        },
        "5fbf44d3fb1e468986d7e1c2b64a3f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73d355f2b37b40e0a7e61dbccc3c436f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7839e369c8b5439d82a3ede210e93556": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80260a16422c4ff8962bb836552dac13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8758105677f24949ad39c3aa650a165a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8de70ed3a3ec4ef2b3ebf29112a990c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99bcf45f49ea465280fd15430b317439": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08390d94f7984820a4f3081bab293101",
            "placeholder": "​",
            "style": "IPY_MODEL_9e348447ac2d431aa4f20d65c7483a0c",
            "value": " 22.7k/22.7k [00:00&lt;00:00, 415kB/s]"
          }
        },
        "9e348447ac2d431aa4f20d65c7483a0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e446983144145f38f3fe6a8696f7e74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22cf733bc0524ac4ab466e0089c03cea",
            "placeholder": "​",
            "style": "IPY_MODEL_7839e369c8b5439d82a3ede210e93556",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "a70404df53a544b4a53fe4a9d0d36c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e446983144145f38f3fe6a8696f7e74",
              "IPY_MODEL_1f445d2cbe164eac9943b9169e05a066",
              "IPY_MODEL_ffeac10b9ea34e66bc5b68302529eafc"
            ],
            "layout": "IPY_MODEL_333ec5cd772843fe8fc041eca37e8d89"
          }
        },
        "a7fa1c194d4048a388e84c8c3342de29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a99f77b726cc4a4a9fd50a3e51960fa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab0e40a656df47c9a7eeb3b661c30a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1b317ee181c40c8a6747c42f7b7b93b",
              "IPY_MODEL_b705ae74ea354e45980fd45fe04f2ce0",
              "IPY_MODEL_543ded25c4b04a1bae801316b41e7b14"
            ],
            "layout": "IPY_MODEL_1a99f2603c0241488694a0ba3a7f395b"
          }
        },
        "ae1a06c80fab4bc29fa587c4c128718c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1b317ee181c40c8a6747c42f7b7b93b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_019243e7224f4e248b46135e2193a267",
            "placeholder": "​",
            "style": "IPY_MODEL_8758105677f24949ad39c3aa650a165a",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "b705ae74ea354e45980fd45fe04f2ce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44889dc130534b3dae0704d39094959c",
            "max": 412,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31d21d02b8ce43acb2968082191ec8ce",
            "value": 412
          }
        },
        "b87464a1460d4ff4b21b428fbeaf7aae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3d3b5b0473b4243a781da67a0e7bc32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3322e17cba64302a6fd52c6ddd7b462": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9bf0034dea1412aafafed56e81bf084": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fbf44d3fb1e468986d7e1c2b64a3f7c",
            "max": 22723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a99f77b726cc4a4a9fd50a3e51960fa9",
            "value": 22723
          }
        },
        "ffeac10b9ea34e66bc5b68302529eafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3322e17cba64302a6fd52c6ddd7b462",
            "placeholder": "​",
            "style": "IPY_MODEL_c3d3b5b0473b4243a781da67a0e7bc32",
            "value": " 486M/486M [00:05&lt;00:00, 154MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
